{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План:\n",
    " \n",
    "0. Задачи NLP \n",
    "0. Пример использование\n",
    "0. Предобработка\n",
    "1. Cуществующие подходы к анализу текстов:\n",
    "    - One-hot-vector\n",
    "    - Bag of words\n",
    "    - Character n-grams\n",
    "    - TF-IDF\n",
    "2. Что такое Word2Vec?\n",
    "    - описание\n",
    "    - косинусная мера\n",
    "    - гипепараметры модели\n",
    "3. Архитектура нейронной сети\n",
    "    - Continuous Bag of Words (CBOW)\n",
    "    - Skip-Gram\n",
    "4. Уменьшение вычислительной сложности\n",
    "    - Negative Sampling\n",
    "6. RNN\n",
    "7. LSTM\n",
    "8. Вывод\n",
    "\n",
    "9. Полезные ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задачи NLP:\n",
    "\n",
    "1. Распознавание речи\n",
    "2. Анализ текста\n",
    "         - Извлечение информации\n",
    "         - Информационный поиск\n",
    "         - Анализ тональности текста\n",
    "         - Вопросно-ответные системы\n",
    "3. Генерирование текста\n",
    "4. Машинный перевод\n",
    "5. Автоматическое реферирование, аннотирование или упрощение текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-02 17:13:04,568 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "/home/denys/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from smart_open import smart_open\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример использования\n",
    "### Dataset\n",
    "Задача sentiment analysis: отзыв и ответ [0,1]  \n",
    "1-позитивный, 0-негативный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data path initialization\n",
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"movie_reviews.csv\"\n",
    "HEADER = True\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(TEXT_DATA_DIR + TEXT_DATA_FILE, nrows=50000)\n",
    "data, labels = df.text, df.label\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = \\\n",
    "    train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>To an entire generation of filmgoers, it just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pixar classic is one of the best kids' movies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Apesar de representar um imenso avanço tecnoló...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>When Woody perks up in the opening scene, it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduced not one but two indelible character...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  To an entire generation of filmgoers, it just ...\n",
       "1      1  Pixar classic is one of the best kids' movies ...\n",
       "2      1  Apesar de representar um imenso avanço tecnoló...\n",
       "3      1  When Woody perks up in the opening scene, it's...\n",
       "4      1  Introduced not one but two indelible character..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fab86fc6a20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADSFJREFUeJzt3H+o3Xd9x/Hna8nqr2FbbSguKUth\nwRGFoYbaIewPO9pUx9I/VCpjDRLMH9bNjcFW90+dWlAY6yyoEGxmK2IsndAw40KpyhistbdW1LQr\nvdRpE6q9mlq3iXZx7/1xP92Oed80J2nS723v8wGH+/1+vp/vuZ8DIc97zvmek6pCkqRZvzL1AiRJ\nq49xkCQ1xkGS1BgHSVJjHCRJjXGQJDXGQZLUGAdJUmMcJEnN+qkXcLouuOCC2rx589TLkKTnjfvu\nu++HVbVhnrnP2zhs3ryZhYWFqZchSc8bSb4771xfVpIkNcZBktQYB0lSYxwkSY1xkCQ1xkGS1BgH\nSVJjHCRJjXGQJDXP209IPx9svu6LUy/hBeXfP/LWqZcgrRk+c5AkNcZBktQYB0lSYxwkSY1xkCQ1\nxkGS1BgHSVJjHCRJjXGQJDXGQZLUGAdJUmMcJEmNcZAkNcZBktQYB0lSYxwkSY1xkCQ1c8UhyZ8l\nOZTk20k+l+TFSS5Ock+SxSSfT3LOmPuisb84jm+euZ/3j/GHklwxM759jC0mue5MP0hJ0qk5aRyS\nbAT+BNhWVa8F1gFXAx8Fbqyq3wSeAHaNU3YBT4zxG8c8kmwd570G2A58Ism6JOuAjwNXAluBd465\nkqSJzPuy0nrgJUnWAy8FHgPeDNw+jt8CXDW2d4x9xvHLkmSM76uqn1fVd4BF4JJxW6yqR6rqKWDf\nmCtJmshJ41BVR4C/Ab7HchSeBO4DflxVx8a0w8DGsb0ReHSce2zMf+Xs+HHnnGhckjSReV5WOp/l\nv+QvBn4deBnLLws955LsTrKQZGFpaWmKJUjSmjDPy0q/B3ynqpaq6r+BLwBvAs4bLzMBbAKOjO0j\nwEUA4/i5wI9mx48750TjTVXtqaptVbVtw4YNcyxdknQ65onD94BLk7x0vHdwGfAA8BXgbWPOTuCO\nsb1/7DOOf7mqaoxfPa5muhjYAnwNuBfYMq5+OoflN633P/uHJkk6XetPNqGq7klyO/B14BhwP7AH\n+CKwL8mHx9jN45Sbgc8kWQSOsvyfPVV1KMltLIflGHBtVf0CIMl7gYMsXwm1t6oOnbmHKEk6VSeN\nA0BVXQ9cf9zwIyxfaXT83J8Bbz/B/dwA3LDC+AHgwDxrkSSdfX5CWpLUGAdJUmMcJEmNcZAkNcZB\nktQYB0lSYxwkSY1xkCQ1xkGS1BgHSVJjHCRJjXGQJDXGQZLUGAdJUmMcJEmNcZAkNcZBktQYB0lS\nYxwkSY1xkCQ1xkGS1BgHSVJjHCRJjXGQJDXGQZLUGAdJUmMcJEmNcZAkNcZBktQYB0lSYxwkSY1x\nkCQ1xkGS1BgHSVKzfuoFSJrIB86degUvLB94cuoVnFE+c5AkNcZBktQYB0lSM1cckpyX5PYk/5bk\nwSS/k+QVSe5M8vD4ef6YmyQ3JVlM8s0kr5+5n51j/sNJds6MvyHJt8Y5NyXJmX+okqR5zfvM4WPA\nP1XVbwG/DTwIXAfcVVVbgLvGPsCVwJZx2w18EiDJK4DrgTcClwDXPx2UMefdM+dtf3YPS5L0bJw0\nDknOBX4XuBmgqp6qqh8DO4BbxrRbgKvG9g7g1lp2N3BeklcBVwB3VtXRqnoCuBPYPo69vKrurqoC\nbp25L0nSBOZ55nAxsAT8fZL7k3wqycuAC6vqsTHn+8CFY3sj8OjM+YfH2DONH15hvEmyO8lCkoWl\npaU5li5JOh3zxGE98Hrgk1X1OuC/+P+XkAAYf/HXmV/eL6uqPVW1raq2bdiw4Wz/Oklas+aJw2Hg\ncFXdM/ZvZzkWPxgvCTF+Pj6OHwEumjl/0xh7pvFNK4xLkiZy0jhU1feBR5O8egxdBjwA7AeevuJo\nJ3DH2N4PXDOuWroUeHK8/HQQuDzJ+eON6MuBg+PYT5JcOq5SumbmviRJE5j36zP+GPhsknOAR4B3\nsRyW25LsAr4LvGPMPQC8BVgEfjrmUlVHk3wIuHfM+2BVHR3b7wE+DbwE+NK4SZImMlccquobwLYV\nDl22wtwCrj3B/ewF9q4wvgC8dp61SJLOPj8hLUlqjIMkqTEOkqTGOEiSGuMgSWqMgySpMQ6SpMY4\nSJIa4yBJaoyDJKkxDpKkxjhIkhrjIElqjIMkqTEOkqTGOEiSGuMgSWqMgySpMQ6SpMY4SJIa4yBJ\naoyDJKkxDpKkxjhIkhrjIElqjIMkqTEOkqTGOEiSGuMgSWqMgySpMQ6SpMY4SJIa4yBJaoyDJKkx\nDpKkxjhIkhrjIElq5o5DknVJ7k/yj2P/4iT3JFlM8vkk54zxF439xXF888x9vH+MP5Tkipnx7WNs\nMcl1Z+7hSZJOx6k8c3gf8ODM/keBG6vqN4EngF1jfBfwxBi/ccwjyVbgauA1wHbgEyM464CPA1cC\nW4F3jrmSpInMFYckm4C3Ap8a+wHeDNw+ptwCXDW2d4x9xvHLxvwdwL6q+nlVfQdYBC4Zt8WqeqSq\nngL2jbmSpInM+8zh74C/AP5n7L8S+HFVHRv7h4GNY3sj8CjAOP7kmP9/48edc6LxJsnuJAtJFpaW\nluZcuiTpVJ00Dkl+H3i8qu57DtbzjKpqT1Vtq6ptGzZsmHo5kvSCtX6OOW8C/iDJW4AXAy8HPgac\nl2T9eHawCTgy5h8BLgIOJ1kPnAv8aGb8abPnnGhckjSBkz5zqKr3V9WmqtrM8hvKX66qPwS+Arxt\nTNsJ3DG29499xvEvV1WN8avH1UwXA1uArwH3AlvG1U/njN+x/4w8OknSaZnnmcOJ/CWwL8mHgfuB\nm8f4zcBnkiwCR1n+z56qOpTkNuAB4BhwbVX9AiDJe4GDwDpgb1UdehbrkiQ9S6cUh6r6KvDVsf0I\ny1caHT/nZ8DbT3D+DcANK4wfAA6cylokSWePn5CWJDXGQZLUGAdJUmMcJEmNcZAkNcZBktQYB0lS\nYxwkSY1xkCQ1xkGS1BgHSVJjHCRJjXGQJDXGQZLUGAdJUmMcJEmNcZAkNcZBktQYB0lSYxwkSY1x\nkCQ1xkGS1BgHSVJjHCRJjXGQJDXGQZLUGAdJUmMcJEmNcZAkNcZBktQYB0lSYxwkSY1xkCQ1xkGS\n1BgHSVJjHCRJjXGQJDUnjUOSi5J8JckDSQ4led8Yf0WSO5M8PH6eP8aT5KYki0m+meT1M/e1c8x/\nOMnOmfE3JPnWOOemJDkbD1aSNJ95njkcA/68qrYClwLXJtkKXAfcVVVbgLvGPsCVwJZx2w18EpZj\nAlwPvBG4BLj+6aCMOe+eOW/7s39okqTTddI4VNVjVfX1sf0fwIPARmAHcMuYdgtw1djeAdxay+4G\nzkvyKuAK4M6qOlpVTwB3AtvHsZdX1d1VVcCtM/clSZrAKb3nkGQz8DrgHuDCqnpsHPo+cOHY3gg8\nOnPa4TH2TOOHVxiXJE1k7jgk+TXgH4A/raqfzB4bf/HXGV7bSmvYnWQhycLS0tLZ/nWStGbNFYck\nv8pyGD5bVV8Ywz8YLwkxfj4+xo8AF82cvmmMPdP4phXGm6raU1Xbqmrbhg0b5lm6JOk0zHO1UoCb\ngQer6m9nDu0Hnr7iaCdwx8z4NeOqpUuBJ8fLTweBy5OcP96Ivhw4OI79JMml43ddM3NfkqQJrJ9j\nzpuAPwK+leQbY+yvgI8AtyXZBXwXeMc4dgB4C7AI/BR4F0BVHU3yIeDeMe+DVXV0bL8H+DTwEuBL\n4yZJmshJ41BV/wKc6HMHl60wv4BrT3Bfe4G9K4wvAK892VokSc8NPyEtSWqMgySpMQ6SpMY4SJIa\n4yBJaoyDJKkxDpKkxjhIkhrjIElqjIMkqTEOkqTGOEiSGuMgSWqMgySpMQ6SpMY4SJIa4yBJaoyD\nJKkxDpKkxjhIkhrjIElqjIMkqTEOkqTGOEiSGuMgSWqMgySpMQ6SpMY4SJIa4yBJaoyDJKkxDpKk\nxjhIkhrjIElqjIMkqTEOkqTGOEiSGuMgSWpWTRySbE/yUJLFJNdNvR5JWstWRRySrAM+DlwJbAXe\nmWTrtKuSpLVrVcQBuARYrKpHquopYB+wY+I1SdKatVrisBF4dGb/8BiTJE1g/dQLOBVJdgO7x+5/\nJnloyvW8gFwA/HDqRZxMPjr1CjSR58W/T/46U69gHr8x78TVEocjwEUz+5vG2C+pqj3AnudqUWtF\nkoWq2jb1OqSV+O9zGqvlZaV7gS1JLk5yDnA1sH/iNUnSmrUqnjlU1bEk7wUOAuuAvVV1aOJlSdKa\ntSriAFBVB4ADU69jjfKlOq1m/vucQKpq6jVIklaZ1fKegyRpFTEOa1iSvUkeT/LtqdciHc+v1JmW\ncVjbPg1sn3oR0vH8Sp3pGYc1rKr+GTg69TqkFfiVOhMzDpJWI79SZ2LGQZLUGAdJq9FcX6mjs8c4\nSFqN/EqdiRmHNSzJ54B/BV6d5HCSXVOvSYLlr9QBnv5KnQeB2/xKneeWn5CWJDU+c5AkNcZBktQY\nB0lSYxwkSY1xkCQ1xkGS1BgHSVJjHCRJzf8C8J4/pEy+zaoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab8c721a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг данных \n",
    "1)Сначала функция будет удалять все символы кроме букв верхнего и нижнего регистра;\n",
    "\n",
    "2)Затем преобразовывает слова к нижнему регистру;\n",
    "\n",
    "3)После чего удаляет стоп слова из текста, т.к. они не несут никакой информации о содержании;\n",
    "\n",
    "4)Лемматизация, процесс приведения словоформы к лемме — её нормальной (словарной) форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-02 16:31:28,356 : INFO : Loading dictionaries from /home/denys/.local/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-12-02 16:31:28,400 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def review_to_wordlist(review):\n",
    "    #1)\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    #2)\n",
    "    words = review_text.lower().split()\n",
    "    #3)\n",
    "    words = [w for w in words if not w in stops]\n",
    "    #4)\n",
    "    words = [morph.parse(w)[0].normal_form for w in words ]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    target_names = ['0', '1']\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    print('accuracy %s' % accuracy_score(target, predictions))\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(vectorizer, classifier, data, labels):\n",
    "    data_features = vectorizer.transform(data)\n",
    "    predictions = classifier.predict(data_features)\n",
    "    target = labels\n",
    "    evaluate_prediction(predictions, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words, n-grams, tf-idf\n",
    "Пример представления предложений:  \n",
    "**11 unique words**: {i, love, data, science, a, scientist, is, often, smarter, than, analyst}\n",
    "\n",
    "Then, our corpus is mapped to feature vectors $T_1=(1,1,1,1,0,0,0,0,0,0,0)$, $T_2=(0,0,2,0,2,1,1,1,1,1,1)$\n",
    "\n",
    "|Text #|i|love|data|science|a|scientist|is  |often|smarter|than|analyst|\n",
    "|------|------|------|------|------|------|------|------|------|------|------|------|\n",
    "|$T_1$|1|1|1|1|0|0|0|0|0|0|0|\n",
    "|$T_2$|0|0|2|0|2|1|1|1|1|1|1|\n",
    "\n",
    "<h2 style=\"font-size:20px; font-family:Verdana; color: #003300\" align=\"left\"> PROS: </h2>\n",
    "* Very intuitive approach, easy to use, understand and apply - you can code it yourself\n",
    "* Built-in support in many scientific/NLP libraries\n",
    "* Memory-efficient sparse format, acceptable by most algorithms \n",
    "* Despite its simplicity, works well, good results could be reached\n",
    "* Fast preprocessing, even on 1 core\n",
    "\n",
    "<h2 style=\"font-size:20px; font-family:Verdana; color: #680000\" align=\"left\"> CONS: </h2>\n",
    "* Huge corpus usually leads to huge vocabulary size (millions of words), even sparse format wouldn't help you (only hashing tricks)\n",
    "* There are other approaches, manageable to catch more details (semantics, relations, structure) - word embeddings etc.\n",
    "* A bag of words is an orderless representation: throwing out spatial relationships between features leads to the fact that simplified model cannot let us to distinguish between sentences, built from the same words while having opposite meanings:\n",
    "<br>\"New episodes **don't** feel like the first - watch it!\" (positive)\n",
    "<br>\"New episodes feel like the first - **don't** watch it!\" (negative)\n",
    "<br>**However, it is somehow treated by increasing the \"length\" of the token (unigrams $\\rightarrow$ bigrams, n-grams etc.), gluing negative particles with next word (not like $\\rightarrow$ not_like), using character n-grams, skip-grams etc.** (see [this section for n-grams details](#3_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 12 ms, total: 14.1 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training\n",
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\", tokenizer=review_to_wordlist,\n",
    "    preprocessor=None, stop_words='english', max_features=3000) \n",
    "train_data_features = count_vectorizer.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 s, sys: 0 ns, total: 1 s\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_data_features, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'able',\n",
       " 'absence',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorbing',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'academy',\n",
       " 'accent']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7578\n",
      "confusion matrix\n",
      " [[1037  714]\n",
      " [ 497 2752]]\n",
      "(row=expected, col=predicted)\n",
      "CPU times: user 1.77 s, sys: 16 ms, total: 1.79 s\n",
      "Wall time: 1.81 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEmCAYAAAAA6gkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHH5JREFUeJzt3Xu8nFV97/HPNzsgyF2DWiFcqgGL\ntApErHdU4BWUAsd6AbwcjgjiMXhBPUWhFKPW26mtHtNqqEi9AKKtPRGjsXpKFQ/YREUkIDEgSBCF\ncL+T4Ld/PGvDw2Zmz8zOTJ7Ze75vXs+LeS6z1prZe/+y1nrWs5ZsExERjzar6QJERAyrBMiIiDYS\nICMi2kiAjIhoIwEyIqKNBMiIiDYSIAdI0paSviHpdklf3Yh0XivpO/0sW1MkvUDSlU2XYxAknS7p\nS+X1LpLukjTW5zyukXRgP9OM9hIgAUlHS1pZfqFvkPQtSc/vQ9KvBJ4IPN72q6aaiO0v2z64D+UZ\nKEmW9NTJrrH9A9t7TjH9ayTdKGmr2rE3SbpgKukNku1f297a9oNNlyWmbuQDpKSTgL8D/poqmO0C\n/D1weB+S3xVYbXtDH9Ka9iTN7kMyY8Db+1AWSRr53//owPbIbsB2wF3Aqya55jFUAfQ3Zfs74DHl\n3AHAWuBdwI3ADcD/KOfeDzwArC95HAucDnyplvZugIHZZf8Y4GrgTuBXwGtrxy+sve+5wArg9vL/\n59bOXQB8APhhSec7wJw2n228/P+rVv4jgJcBq4FbgPfVrt8fuAi4rVz7aWDzcu775bPcXT7va2rp\n/wXwW+CL48fKe55S8ti37D8ZuAk4oE15rwFOLu/Zvhx7E3BBD9/Nh8p3cy/w1HLsg8D/L+X+BvB4\n4MvAHSWN3WppfBK4rpz7MfCC2rmHfr71ny3wnJL2+HYfcE25blb5TFcBNwPnAY+rpfl64Npy7pTy\nHRzY9N/OqGyNF6DRDw8LgA2UANXmmkXAxcATgB3LH9IHyrkDyvsXAZuVwHIPsEM5/9AfTJv9+h/R\nVuWPbs9y7g+Ap5fXx1ACJPA44NbyhzMbOKrsP76cv6D8se0BbFn2P9Lms42X/7RS/uOoAtTZwDbA\n00sg2b1cvx/wpyXf3YArgHfU0jPw1Bbpf5TqH5otqQXIcs1xwOXAY4HlwP+e5GdxDXAg8C/AB8ux\nhwJkl9/Nr8vnml0+8wXAGqpgvV0py+qSz2zgC8Dna2V4HVUAnU31D+NvgS0m/nyZ8I9f7f2bAf8B\nfLjsv53q92vn8h19FjinnNuLKqC+sJz7RPk+EyA30TbqTYzHA+s8eRP4tcAi2zfavomqZvj62vn1\n5fx628uofqGn1McG/B7YW9KWtm+wvarFNS8Hfmn7i7Y32D4H+AXwZ7VrPm97te17qWokz5wkz/XA\nh2yvB84F5gCftH1nyf9y4BkAtn9s++KS7zVUf8wv6uIz/ZXt+0t5HsH2GVQB6kdU/yic0iE9qAL6\niZJ2nHC8m+/mLNuryvn15djnbV9l+3bgW8BVtr9bfi++CuxTK++XbN9c3v83VIGrl5/3p6hq9uOf\n8wTgFNtrbd9PFWRfWbojXgmcb/v75dxfUn2fsYmMeoC8GZjToW/syVRNnHHXlmMPpTEhwN4DbN1r\nQWzfTdUsPQG4QdI3JT2ti/KMl2mn2v5veyjPzX74RsJ4APtd7fy94++XtIek8yX9VtIdVP22cyZJ\nG+Am2/d1uOYMYG/g/5RAMCnblwHnUzVN67r5bq5rkeTEz9vy8wNIerekK8rIhNuoap2dvoPx976Z\nqgZ9tO3xQLcr8HVJt5X0rgAepOoPf3K9vOV35OZu8or+GPUAeRFwP1W/Wzu/ofolHrdLOTYVd1M1\nJcc9qX7S9nLbB1HVpH5BFTg6lWe8TNdPsUy9+Aeqcs2zvS3wPkAd3jPpdFGStqbq1/0ccLqkx3VZ\nlr+iap7Xg183382Up6+S9AKq/tpXU3WjbE/V19npOxh/7weAw23fUTt1HXCI7e1r2xa2r6fq551b\nS+OxVK2e2ERGOkCWJtVpwGJJR0h6rKTNJB0i6WPlsnOAUyXtKGlOuf5LU8zyEuCFZYzcdsB7x09I\neqKkw8sQlvupmuqtmlPLgD3K0KTZkl5D1Vd1/hTL1IttqPpJ7yq127dMOP874A97TPOTwErbbwK+\nCXymmzfZXgN8BXhb7fCgv5ttqPoAbwJmSzoN2LbTmyTNperqeIPt1RNOfwb4kKRdy7U7ShofQfE1\n4FBJz5e0OVVf90j/zW5qI/9ll36kk4BTqX7xrwMWAv9aLvkgsBK4FPg58JNybCp5/RvVH/WlVHdA\n63+4s0o5fkN1l/ZFPDoAYftm4FCqGwQ3U9VoDrW9bipl6tG7gaOp+tDOoPosdacD/1Sai6/ulFgJ\nBAt4+HOeBOwr6bVdlmcR1c0tYJN8N8uBb1PdxLmW6m50qyb7RC+lajJ/rYy1vUvSeP/yJ4GlwHck\n3Ul1w+bZ5fOsAt5KddPsBqobTmv79FmiC7IzYW5ERCsjX4OMiGgnATIioo0EyIiINhIgIyLa6Mfk\nAX2z2dbbe4sdntT5wpgW5myzedNFiD5Z95u13HnbLR3He/ZibNtd7Q2PeriqLd9703LbC/pZhk6G\nKkBuscOT2O9dn2u6GNEnb3z+Lk0XIfrkr97w8r6n6Q338pg9O44Ge8h9lyzu6omlfhqqABkRo0Qw\n5DPOJUBGRDMEqK+t9r5LgIyI5qQGGRHRimBWX5fs6bsEyIhoTprYEREtiDSxIyJaU2qQERFtpQYZ\nEdFGapAREa1koHhERGvTYKD4cIfviJjZNKv7rVNS0gJJV0paI2niipeUtaD+XdJPJV0q6WWd0kwN\nMiIaIhjrz0BxSWPAYuAgqnV7Vkhaavvy2mWnAufZ/gdJe1Et8rbbZOmmBhkRzRgfB9mfGuT+wBrb\nV9t+ADgXOHzCNebhVSi3o4vlm1ODjIjm9NYHOUfSytr+EttLyuudeOQKk2spq0PWnE61euSJVKth\nHtgpwwTIiGhIz3ex19mevxEZHgWcZftvJD0H+KKkvW23Wn8eSICMiCb17y729cDc2v7O5VjdsVTr\nsGP7IklbAHOAG9slmj7IiGhO//ogVwDzJO0uaXPgSGDphGt+DbwUQNIfAVsAN02WaGqQEdEM9e9Z\nbNsbJC0ElgNjwJm2V0laBKy0vRR4F3CGpHdS3bA5xrYnSzcBMiKa08cnaWwvoxq6Uz92Wu315cDz\nekkzATIimjPkT9IkQEZEQ/IsdkREayJLLkREtJYaZEREe+mDjIhoIzXIiIg2UoOMiGhB6YOMiGgv\nNciIiNaUABkR8WjVkjQJkBERjyahWQmQEREtpQYZEdFGAmRERBsJkBERrahsQywBMiIaIZQaZERE\nOwmQERFtJEBGRLSRABkR0Upu0kREtCbErFmZzScioqU0sSMi2hnu+Mhw128jYuZSVYPsduuYnLRA\n0pWS1kg6ucX5v5V0SdlWS7qtU5qpQUZEY/rVxJY0BiwGDgLWAiskLbV9+fg1tt9Zu/5EYJ9O6aYG\nGRGN6WMNcn9gje2rbT8AnAscPsn1RwHndEo0NciIaESfHzXcCbiutr8WeHbLfKVdgd2B/9cp0QTI\niGhOb/FxjqSVtf0ltpdMIdcjga/ZfrDThQmQEdEM9dwHuc72/Dbnrgfm1vZ3LsdaORJ4azcZpg8y\nIhrTxz7IFcA8SbtL2pwqCC5tkd/TgB2Ai7opX2qQEdGYfq1JY3uDpIXAcmAMONP2KkmLgJW2x4Pl\nkcC5tt1NugmQEdGYfj5JY3sZsGzCsdMm7J/eS5oDbWJ3GrgZEaOrl+Z1U48kDqwG2c3AzYgYbcP+\nLPYga5C9DtyMiBEz7DXIQQbIVgM3d5p4kaTjJa2UtHL93R0fjYyImUQ9bA1ofJiP7SW259uev9lW\n2zddnIjYhIa9BjnIu9i9DNyMiFHT+0DxTW6QNciuBm5GxGgSIHW/NWFgNch2AzcHlV9ETDdiVp8G\nig/KQAeKtxq4GRExbtib2HmSJiKa0WDTuVsJkBHRCMFoN7EjIiaTGmRERBvpg4yIaCV9kBERrVXj\nIIc7QiZARkRDmnuEsFsJkBHRmCGPjwmQEdEQZZhPRERL6YOMiJjEkMfHBMiIaE5qkBERbQx5fEyA\njIiGTIMJcxMgI6IR4xPmDrMEyIhoyPAPFG980a6IGF39XHJB0gJJV0paI+nkNte8WtLlklZJOrtT\nmqlBRkQz+jhQXNIYsBg4iGqJ6RWSltq+vHbNPOC9wPNs3yrpCZ3STQ0yIhoxPlC8T8u+7g+ssX21\n7QeAc4HDJ1xzHLDY9q0Atm/slGgCZEQ0pscAOUfSytp2fC2pnYDravtry7G6PYA9JP1Q0sWSFnQq\nX5rYEdGYHu/RrLM9fyOymw3MAw4Adga+L+mPbd/W7g2pQUZEY/rYxL4emFvb37kcq1sLLLW93vav\ngNVUAbOtBMiIaEYPd7C7qGmuAOZJ2l3S5sCRwNIJ1/wrVe0RSXOomtxXT5ZomtgR0Qj1cRyk7Q2S\nFgLLgTHgTNurJC0CVtpeWs4dLOly4EHgPbZvnizdBMiIaEw/x4nbXgYsm3DstNprAyeVrSsJkBHR\nmFlD/iRNAmRENGbI42MCZEQ0Q4KxLLkQEdHasE9W0TZAStp2sjfavqP/xYmIUTLk8XHSGuQqwFSP\nTI4b3zewywDLFREznKiG+gyztgHS9tx25yIi+mHIuyC7e5JG0pGS3lde7yxpv8EWKyJmvB4eM2yq\nr7JjgJT0aeDFwOvLoXuAzwyyUBExGvo5Ye4gdHMX+7m295X0UwDbt5RnHSMipkzMjIHi6yXNorox\ng6THA78faKkiYiQMeXzsqg9yMfDPwI6S3g9cCHx0oKWKiJEw7H2QHWuQtr8g6cfAgeXQq2xfNthi\nRcRMN5OepBkD1lM1szOHZET0xXCHx+7uYp8CnAM8mWqW3rMlvXfQBYuImW/aN7GBNwD72L4HQNKH\ngJ8CHx5kwSJiZqvuYjddisl1EyBvmHDd7HIsImLqGqwZdmuyySr+lqrP8RZglaTlZf9gqvUfIiI2\nypDHx0lrkON3qlcB36wdv3hwxYmIUTJta5C2P7cpCxIRo2VG9EFKegrwIWAvYIvx47b3GGC5ImIE\nDHsNspsxjWcBn6cK+IcA5wFfGWCZImIESDAmdb01oZsA+VjbywFsX2X7VKpAGRGxUWbCbD73l8kq\nrpJ0AnA9sM1gixURo2AmNLHfCWwFvA14HnAc8MZBFioiRkM/a5CSFki6UtIaSSe3OH+MpJskXVK2\nN3VKs5vJKn5UXt7Jw5PmRkRsFKG+zQcpaYxq5rGDgLXACklLbV8+4dKv2F7YbbqTDRT/OmUOyFZs\nv6LbTCIiHqW/fYv7A2tsXw0g6VzgcGBigOzJZDXIT29MwlOxxxO25tsLn7eps40B2eFZXf9DHUPu\n/mt+O5B0e+yDnCNpZW1/ie0l5fVOwHW1c2uBZ7dI488lvRBYDbzT9nUtrnnIZAPFv9ddmSMipqbH\nuRPX2Z6/Edl9AzjH9v2S3gz8E/CSyd6QuR0johGir9OdXQ/Ul6reuRx7iO2bbd9fdv8R6Lg6awJk\nRDRmlrrfOlgBzJO0e1lU8Ehgaf0CSX9Q2z0MuKJTot3OKI6kx9Sib0TERunnkgu2N0haCCynWgHh\nTNurJC0CVtpeCrxN0mHABqpZyo7plG43z2LvD3wO2A7YRdIzgDfZPnHKnyYigv5OVmF7GbBswrHT\naq/fC/S0GkI3TexPAYcCN5dMfga8uJdMIiJamQmPGs6yfe2ETtIHB1SeiBgR1XRnw/2oYTcB8rrS\nzHYZrX4i1RiiiIiNMux3ibsJkG+hambvAvwO+G45FhGxUYa8AtnVs9g3Ut0yj4joG6l/z2IPSjd3\nsc+gxTPZto8fSIkiYmQMeXzsqon93drrLYD/xiOfeYyImJJpvyaN7UcsryDpi8CFAytRRIwE0b+B\n4oPS9ZM0NbsDT+x3QSJixHT3CGGjuumDvJWH+yBnUT2i86jZeiMieiWGO0JOGiBVjQ5/Bg/PivF7\n220n0Y2I6NZ0WBd70nGaJRgus/1g2RIcI6Jv+jibz2DK18U1l0jaZ+AliYiR08f5IAdisjVpZtve\nAOxDtQDOVcDdVDVj2953E5UxImag6dDEnqwP8j+BfakmloyI6K8GZ+np1mQBUgC2r9pEZYmIETOd\nHzXcUdJJ7U7a/sQAyhMRI2K6N7HHgK1hyAcqRcQ0JcamcQ3yBtuLNllJImKkVKsaNl2KyXXsg4yI\nGIhp/qjhSzdZKSJiJE3bmzS2b9mUBYmI0TLdm9gREQM1bWuQERGDNuTxMQEyIpohhn9Vw2EvX0TM\nVOrvZBWSFki6UtIaSW3nrJX055IsaX6nNBMgI6Ix6mGbNB1pDFgMHALsBRwlaa8W120DvB34UTfl\nS4CMiEYIGJO63jrYH1hj+2rbDwDnAoe3uO4DwEeB+7opYwJkRDRG6n4D5khaWdvqS0/vxCNXW11b\njtXy0r7AXNvf7LZ8uUkTEQ3peSLcdbY79hu2zEmaBXwCOKaX9yVARkQj+nwX+3pgbm1/Zx5eSwtg\nG2Bv4IISlJ8ELJV0mO2V7RJNgIyIxvRxKYUVwDxJu1MFxiOBo8dP2r4dmFPL9wLg3ZMFR0gfZEQ0\nqF93scvyMAuB5cAVwHm2V0laJGnKqyKkBhkRzVBfa5DYXgYsm3DstDbXHtBNmgmQEdGI6fAkTQJk\nRDSmqeVcu5UAGRGNmc4T5kZEDEzVxB7uCJkAGRGNGfIWdgJkRDRFKDXIiIjWUoOMiGghfZAREe0o\nNciIiLYSICMi2shNmoiIFkQGikdEtJV1sSMi2kgTOyKihenQxB7YbEOSzpR0o6TLBpVHRExn6um/\nJgxyOrazgAUDTD8iprMeVjRsqqtyYAHS9veBWwaVfkRMf/1acmFQGu+DLGvbHg8wd5ddGi5NRGwq\nVR/kcHdCNj7jue0ltufbnr/jnB2bLk5EbEKpQUZEtDPcFcgEyIhozsg2sSWdA1wE7ClpraRjB5VX\nRExPI9vEtn3UoNKOiBliuCuQzd+kiYjRVNUM+zdQXNICSVdKWiPp5BbnT5D0c0mXSLpQ0l6d0kyA\njIhm9HGguKQxYDFwCLAXcFSLAHi27T+2/UzgY8AnOhUxATIiGtPHPsj9gTW2r7b9AHAucHj9Att3\n1Ha3Atwp0dzFjojm9NYHOUfSytr+EttLyuudgOtq59YCz35UdtJbgZOAzYGXdMowATIiGtLzJBTr\nbM/fmBxtLwYWSzoaOBX475NdnyZ2RDSmj5NVXA/Mre3vXI61cy5wRKdEEyAjohG99D92Uc9cAcyT\ntLukzYEjgaWPyE+aV9t9OfDLTommiR0RjVGfnqSxvUHSQmA5MAacaXuVpEXASttLgYWSDgTWA7fS\noXkNCZAR0aB+PmloexmwbMKx02qv395rmgmQEdGYIX+QJgEyIhrS5EPWXUqAjIjGZFXDiIgWRHNr\nzXQrATIiGjPk8TEBMiIaNOQRMgEyIhqTPsiIiDZmDXd8TICMiAYlQEZEPNr4jOLDLAEyIprR3Sw9\njUqAjIjGDHl8TICMiAYNeYRMgIyIhvQ8o/gmlwAZEY1JH2RERAvTYDKfBMiIaNCQR8gEyIhozKwh\nb2MnQEZEY4Y7PCZARkRTMlA8ImIywx0hEyAjohGZUTwiYhJDHh8TICOiOcNeg5zVdAEiYnSph/86\npiUtkHSlpDWSTm5x/iRJl0u6VNL3JO3aKc0EyIhojnrYJktGGgMWA4cAewFHSdprwmU/Bebb/hPg\na8DHOhUvATIiGtOn+AiwP7DG9tW2HwDOBQ6vX2D7323fU3YvBnbulGj6ICOiEVJfn6TZCbiutr8W\nePYk1x8LfKtTogmQEdGc3uLjHEkra/tLbC/pOUvpdcB84EWdrk2AjIjG9Fh/XGd7fptz1wNza/s7\nl2OPzE86EDgFeJHt+ztlmD7IiGiM1P3WwQpgnqTdJW0OHAksfWRe2gf4LHCY7Ru7KV9qkBHRkP7N\nKG57g6SFwHJgDDjT9ipJi4CVtpcCHwe2Br6qKuL+2vZhk6WbABkRjej3o4a2lwHLJhw7rfb6wF7T\nTBM7IqKN1CAjojHD/qhhAmRENCarGkZEtFANFG+6FJNLgIyI5iRARkS0liZ2REQbuUkTEdHGkMfH\nBMiIaNCQR8gEyIhozLD3Qcp202V4iKSbgGubLscmMAdY13Qhoi9G5We5q+0d+5mgpG9TfX/dWmd7\nQT/L0MlQBchRIWnlJNM2xTSSn+XMlmexIyLaSICMiGgjAbIZPU8TH0MrP8sZLH2QERFtpAYZEdFG\nAmRERBsJkBERbSRAbgKS9pT0HEmbSRprujyx8fJzHA25STNgkl4B/DXVGr3XAyuBs2zf0WjBYkok\n7WF7dXk9ZvvBpssUg5Ma5ABJ2gx4DXCs7ZcC/5dqcfO/kLRto4WLnkk6FLhE0tkAth9MTXJmS4Ac\nvG2BeeX114Hzgc2Ao6Vhnw0vxknaClgIvAN4QNKXIEFypkuAHCDb64FPAK+Q9ALbvwcuBC4Bnt9o\n4aIntu8G3gicDbwb2KIeJJssWwxOAuTg/QD4DvB6SS+0/aDts4EnA89otmjRC9u/sX2X7XXAm4Et\nx4OkpH0lPa3ZEka/ZT7IAbN9n6QvAwbeW/6I7geeCNzQaOFiymzfLOnNwMcl/QIYA17ccLGizxIg\nNwHbt0o6A7icquZxH/A6279rtmSxMWyvk3QpcAhwkO21TZcp+ivDfDax0qHv0h8Z05ikHYDzgHfZ\nvrTp8kT/JUBGbARJW9i+r+lyxGAkQEZEtJG72BERbSRARkS0kQAZEdFGAmRERBsJkDOEpAclXSLp\nMklflfTYjUjrAEnnl9eHSTp5kmu3l/Q/p5DH6ZLe3e3xCdecJemVPeS1m6TLei1jRALkzHGv7Wfa\n3ht4ADihflKVnn/etpfa/sgkl2wP9BwgI6aDBMiZ6QfAU0vN6UpJXwAuA+ZKOljSRZJ+UmqaWwNI\nWiDpF5J+ArxiPCFJx0j6dHn9RElfl/Szsj0X+AjwlFJ7/Xi57j2SVki6VNL7a2mdImm1pAuBPTt9\nCEnHlXR+JumfJ9SKD5S0sqR3aLl+TNLHa3m/eWO/yBhtCZAzjKTZVI++/bwcmgf8ve2nA3cDpwIH\n2t6XavLekyRtAZwB/BmwH/CkNsl/CvgP288A9gVWAScDV5Xa63skHVzy3B94JrCfpBdK2g84shx7\nGfCsLj7Ov9h+VsnvCuDY2rndSh4vBz5TPsOxwO22n1XSP07S7l3kE9FSnsWeObaUdEl5/QPgc1Qz\nBl1r++Jy/E+BvYAflqkoNwcuAp4G/Mr2LwHKDDXHt8jjJcAb4KEpvm4vj9vVHVy2n5b9rakC5jbA\n123fU/JY2sVn2lvSB6ma8VsDy2vnziuPa/5S0tXlMxwM/Emtf3K7kvfqLvKKeJQEyJnjXtvPrB8o\nQfDu+iHg32wfNeG6R7xvIwn4sO3PTsjjHVNI6yzgCNs/k3QMcEDt3MRHwFzyPtF2PZAiabcp5B2R\nJvaIuRh4nqSnQjVLtqQ9gF8Au0l6SrnuqDbv/x7wlvLeMUnbAXdS1Q7HLQfeWOvb3EnSE4DvA0dI\n2lLSNlTN+U62AW4oS1e8dsK5V0maVcr8h8CVJe+3lOuRtEeZCTxiSlKDHCG2byo1sXMkPaYcPtX2\naknHA9+UdA9VE32bFkm8HVgi6VjgQeAtti+S9MMyjOZbpR/yj4CLSg32Lqqp3X4i6SvAz4AbgRVd\nFPkvgR8BN5X/18v0a+A/qZa0OKHMu/mPVH2TP1GV+U3AEd19OxGPlskqIiLaSBM7IqKNBMiIiDYS\nICMi2kiAjIhoIwEyIqKNBMiIiDYSICMi2vgvROFHV5nBBhYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8013880d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "predict(count_vectorizer, logreg, data_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 112 ms, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_gram_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=([1,3]),\n",
    "    tokenizer=None,    \n",
    "    preprocessor=None,                               \n",
    "    max_features=3000) \n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "\n",
    "train_data_features = n_gram_vectorizer.fit_transform(data_train)\n",
    "\n",
    "logreg = logreg.fit(train_data_features, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adults',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'affair',\n",
       " 'affecting',\n",
       " 'after',\n",
       " 'after the',\n",
       " 'again',\n",
       " 'against',\n",
       " 'against the']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer.get_feature_names()[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7622\n",
      "confusion matrix\n",
      " [[1041  710]\n",
      " [ 479 2770]]\n",
      "(row=expected, col=predicted)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEmCAYAAAAA6gkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJMiIiBaSICMiWkiCjIhoIQkyIqKF/wR0QFDZIf9wvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7fcf0f9dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(n_gram_vectorizer, logreg, data_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность неможко подросла, увеличивая размер ngram можно еще поднять точность. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "$$  \n",
    "    \\begin{cases} TF(w,T)=n_{Tw} \\\\ IDF(w, T)= log{\\frac{N}{n_{w}}}\\end{cases} \\implies \n",
    "    TF\\text{-}IDF(w, T) = n_{Tw}\\ log{\\frac{N}{n_{w}}} \\ \\ \\ \\ \\forall w \\in W\n",
    "$$\n",
    "\n",
    "<br> where $T$ corresponds to current document (text), \n",
    "<br>$w$ - selected word in document T, \n",
    "<br>$n_{Tw}$ - number of occurences of $w$ in text $T$, \n",
    "<br>$n_{w}$ - number of documents, containing word $w$, \n",
    "<br> $N$ - total number of documents in a corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 8 ms, total: 17.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_vect = TfidfVectorizer(\n",
    "    min_df=1, tokenizer=review_to_wordlist,\n",
    "    preprocessor=None)\n",
    "\n",
    "train_data_features = tf_vect.fit_transform(data_train)\n",
    "\n",
    "logreg = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(train_data_features, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7328\n",
      "confusion matrix\n",
      " [[1066  685]\n",
      " [ 651 2598]]\n",
      "(row=expected, col=predicted)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEmCAYAAAAA6gkZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAFpZlbAAWlmVsABaWZW4P8D5n2UNw1zE+cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7fd0428be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(tf_vect, logreg, data_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вариаинт можно попробовать добавить ngrams - должно повысить точность. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "Word2Vec – одна из технологий анализа семантики естественных языков, которая основана на векторном представлении слов согласно их семантической близости.\n",
    "\n",
    "Суть алгоритма заключается в том, что он каждому слову сопоставляет числовой вектор определенной длины таким образом, чтобы близкие слова соответствуют близким векторам. Мерой близости слов выступает их контекстная близость т.е. близкие слова встречаются в тексте рядом с одинаковыми словами. \n",
    "\n",
    "<img src=\"pictures/4.png\">\n",
    "\n",
    "Расстоянием между векторами измеряется при помощи косинусного сходства (cosine similarity).Косинусная мера между векторами x и y длины n вычисляется по формуле:\n",
    "\n",
    "$$cos(\\theta)=\\frac{(x,y)}{|x| |y|}=\\frac{\\sum\\limits_{i=1}^{n}x_i y_i}{\\sqrt{\\sum\\limits_{i=1}^{n}x_i^2} \\sqrt{\\sum\\limits_{i=1}^{n}y_i^2}}$$\n",
    "\n",
    "Обучаясь, Word2Vec максимизирует косинусную меру близости между векторами слов, которые встречаются в похожих контекстах и минимизирует косинусную меру между словами, которые не встречаются рядом.Word2Vec получает на вход слово, а на выход передает координаты вектора, соответствующие данному слову.\n",
    "\n",
    "Полученные вектора можно складывать или вычитать друг из друга, сохраняя семантические связи.\n",
    "\n",
    "В Word2Vec можно использовать две различных архитектуры нейронной сети с помощью которых осуществляется перевод слова в вектор: \n",
    "\n",
    "- Continuous Bag of Words (CBOW);\n",
    "- Skip-gram.\n",
    "\n",
    "Еще один важный гиперпараметр ‘size’ - размерность векторов, соответствующих словам. Если его величина мала, то модель получается грубой и плохо отображает связь между словами внутри данного массива текстов. А при большом значении роль машинного обучения теряется, и сопоставление словам векторов может превратиться в унитарное кодирование слов (one-hot encoding).\n",
    "\n",
    "# Continuous Bag of Words (CBOW)\n",
    "Когда данных мало лучше использовать CBOW, т.к. она менее склонна к переобучению. Отметич, что CBOW работает быстрее, чем skip-gram, но хуже учитывает редкие слова. В данном подходе нейронная сеть предсказывает исходное слово по его контексту ($h$ соседним словам). Нейронная сеть состоит из трех слоев: входной, скрытый, выходной.\n",
    "\n",
    "На вход сети подаются $h$ векторов размерности $V$: $x_i=(x_i^1,x_i^2,...,x_i^V), i=1...h$, где $x_i^j=1$, если данное слово является $j$-ым словом из словаря, $x_i^k=0$ для $k\\neq j$. На выходе имеем один вектор размерности $V$: $y=(y^1,y^2,...,y^V)$. На обучающейся выборке $y^j=1$, если предсказываемое слово является $j$-ым словом из словаря, $y^k=0$ для $k\\neq j$. То есть нейронная сеть имеет $h\\times V$ нейронов на входном слое и $V$ нейронов на выходном слое.\n",
    "\n",
    "На скрытом слое сети $N$ нейронов. Именно с помощью весов, расставленных перед нейронами этого слоя, мы получим координаты векторных представлений слов. Функция активации на скрытом уровне — линейная, на выходном уровне — софтмакс (softmax).\n",
    "\n",
    "Сначала рассмотрим простейший случай, когда  $h=1$.То есть будем предсказывать слово $y$ только по одному его соседу $x$. Тогда архитектура сети примет вид, изображенный на схеме:\n",
    "\n",
    "<img src=\"pictures/h1.png\">\n",
    "\n",
    "Обозначим веса между входным и скрытым уровнями за $W$ - матрица размерности $V\\times N$. Учитывая, что функция активации на скрытом слое линейная, вектор выходов размерности $N$ на скрытом уровне имеет вид: $$v=Wx$$\n",
    "\n",
    "Обозначим веса между скрытым и выходным уровнями за $W'$ - матрица размерности $N\\times V$. Пусть $w'_j$ - $j$-ая строка матрицы $W'$. Тогда входной сигнал $j$-ого нейрона на выходном уровне имеет вид: $$u^j=w'_j v$$\n",
    "\n",
    "Так как на выходном уровне используется функция активации softmax, выходной сигнал на  $j$-ом нейроне выходного слоя принимает вид:\n",
    "\n",
    "$$y^j=\\frac{exp(u^j)}{\\sum\\limits_{k=1}^{V} exp(u^k)}$$\n",
    "\n",
    "Такое представление имеет вероятностную интерпретацию. Пусть $w_I$ - входное слово, а $w_O$ - выходное слово. Обозначим получившееся выражение для $y^j$ за $p(w_j|w_I)$. Обучаясь, сеть максимизирует $y^{j*}=\\frac{exp(u^{j*})}{\\sum\\limits_{k=1}^{V} exp(u^k)}=p(w_O|w_I)$, при условии что $w_O = w_{j*}$.\n",
    "\n",
    "Сеть обучается методом обратного распространения ошибки (backpropagation), то есть сначала корректируются веса $W'$, а затем $W$. Минимизируемый функционал потерь имеет вид:\n",
    "$$L=-log \\ p(w_O|w_I)$$\n",
    "\n",
    "После того, как процесс обучения завершился, $V$ строк длины $N$ матрицы $W'$ дадут нам  координаты векторов, представляющих слова из словаря.\n",
    "\n",
    "Теперь рассмотрим общий случай для произвольного $h$. То есть будем предсказывать слово $y$ только по соседним словам $x_i, i=1...h$. Тогда архитектура сети примет вид, изображенный на схеме:\n",
    "\n",
    "\n",
    "<img src=\"pictures/5.png\">\n",
    "\n",
    "Обозначим веса между входными и скрытым уровнями за $W$ - матрица размерности $Vh\\times N$. Учитывая, что функция активации на скрытом слое линейная, вектор выходов размерности $N$ на скрытом уровне имеет вид: $$v= \\frac{1}{h} W (x_1+...+x_h)$$\n",
    "\n",
    "Далее, аналогично случаю $h=1$ получаем входной сигнал $j$ -ого нейрона на выходном слое сети $$u^j=w'_j v,$$\n",
    "где $w'_j$ - $j$-ая строка матрицы весов между скрытым и выходным уровнями $W'$.\n",
    "\n",
    "Пусть входные слова (соседние слова предсказываемого) - $w_{I,1},...,w_{I,h}$. Тогда выходной сигнал на  $j$-ом нейроне выходного слоя принимает вид:\n",
    "\n",
    "$$y^j=\\frac{exp(u^j)}{\\sum\\limits_{k=1}^{V} exp(u^k)}=p(w_j|w_{I,1},...,w_{I,h})$$\n",
    "\n",
    "Как и ранее, обучаясь, сеть максимизирует $y^{j*}=\\frac{exp(u^{j*})}{\\sum\\limits_{k=1}^{V} exp(u^k)}=p(w_O= w_{j*}|w_{I,1},...,w_{I,h})$, при условии что $w_O = w_{j*}$.\n",
    "\n",
    "Минимизируемый функционал потерь имеет вид:\n",
    "$$L=-log \\ p(w_O|w_{I,1},...,w_{I,h})$$\n",
    "\n",
    "# Skip-Gram\n",
    "В случае когда данных много, то лучше использовать Skip-Gram. Данный подход обратен CBOW: по заданному слову предсказывается его контекст ($h$ соседних слов). Сеть также состоит из трех слоев: входной, скрытый, выходной.\n",
    "\n",
    "На скрытом слое сети, как и ранее, $N$ нейронов, а функции активации на скрытом уровне — линейная, на выходном уровне — софтмакс (softmax).\n",
    "\n",
    "Схема архитектуры сети изображена на схеме:\n",
    "\n",
    "\n",
    "<img src=\"pictures/6.png\">\n",
    "\n",
    "# Уменьшение вычислительной сложности\n",
    "### Negative Sampling\n",
    "Идея этого метода состоит в том, чтобы пересчитывать функционал потерь не по всем словам из словаря, а только по меньшему количеству слов. Тем самым пересчитываться будут вектора не всех слов из словаря, а только некоторого подмножества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "натренированая модель на Google News corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "wv = KeyedVectors.load_word2vec_format(\n",
    "    \"/home/denys/word2vec-GoogleNews-vectors/GoogleNews-vectors-negative300.bin.gz\",\n",
    "    binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('females', 0.656234860420227), ('lesbians', 0.5949513912200928), ('woman', 0.5667258501052856)]\n"
     ]
    }
   ],
   "source": [
    "model.most_similar(\"women\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проекция на дву мерную плоскость"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что Word2Vec не единственная технология. Например:\n",
    "\n",
    "1)[Glove](https://nlp.stanford.edu/projects/glove/), тут их репозиторий с объяснением работы,инструкцией и предобученными моедлями на вики и твиттере;\n",
    "\n",
    "2)[AdaGram](https://github.com/lopuhin/python-adagram) репозиторий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь используя Word2vec embedding преобразуем предложения в векторное представления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      " To an entire generation of filmgoers, it just might represent the most significant leap in storytelling that they will ever see...\n",
      "Sentence in indexes:\n",
      " [23, 2, 96, 1195, 21, 1, 36, 2557, 479, 12, 2, 171, 32, 121, 35, 81, 326, 11, 8]\n",
      "Sentence fitted to max length:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0   23    2   96 1195   21    1   36 2557  479\n",
      "   12    2  171   32  121   35   81  326   11    8]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# initialize dictionary size and maximum sentence length\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "print(\"Original sentence:\\n\", data_train[0])\n",
    "\n",
    "# create a dictionary with Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "# replacing words with their indexes from our dictionary\n",
    "X_train = tokenizer.texts_to_sequences(data_train)\n",
    "X_val = tokenizer.texts_to_sequences(data_val)\n",
    "\n",
    "print(\"Sentence in indexes:\\n\", X_train[0])\n",
    "\n",
    "# fit each sentence to max length\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Sentence fitted to max length:\\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with vector representation: 400000\n"
     ]
    }
   ],
   "source": [
    "# path to embeddings file\n",
    "EMBEDDINGS_DIR = BASE_DIR + 'embeddings'\n",
    "EMBEDDINGS_FILE = 'glove.6B.50d.txt'\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# choose only 10000 words from our dictionary\n",
    "first_10000 = {k: v for k, v in tokenizer.word_index.items() if v < 10000}\n",
    "\n",
    "# upload embeddings\n",
    "embeddings = {}\n",
    "with zipfile.ZipFile(os.path.join(EMBEDDINGS_DIR, EMBEDDINGS_FILE+'.zip')) as myzip:\n",
    "    with myzip.open(EMBEDDINGS_FILE) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0].decode('UTF-8')\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        del values, word, coefs, line\n",
    "print(\"Number of words with vector representation:\", len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embeddings matrix where each row is word index\n",
    "\n",
    "embedding_matrix = np.zeros((tokenizer.num_words, EMBEDDING_DIM))\n",
    "for word, i in first_10000.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком виде теперь представляется каждое слово в словаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.18000013e-01,   2.49679998e-01,  -4.12420005e-01,\n",
       "         1.21699996e-01,   3.45270008e-01,  -4.44569997e-02,\n",
       "        -4.96879995e-01,  -1.78619996e-01,  -6.60229998e-04,\n",
       "        -6.56599998e-01,   2.78430015e-01,  -1.47670001e-01,\n",
       "        -5.56770027e-01,   1.46579996e-01,  -9.50950012e-03,\n",
       "         1.16579998e-02,   1.02040000e-01,  -1.27920002e-01,\n",
       "        -8.44299972e-01,  -1.21809997e-01,  -1.68009996e-02,\n",
       "        -3.32789987e-01,  -1.55200005e-01,  -2.31309995e-01,\n",
       "        -1.91809997e-01,  -1.88230002e+00,  -7.67459989e-01,\n",
       "         9.90509987e-02,  -4.21249986e-01,  -1.95260003e-01,\n",
       "         4.00710011e+00,  -1.85939997e-01,  -5.22870004e-01,\n",
       "        -3.16810012e-01,   5.92130003e-04,   7.44489999e-03,\n",
       "         1.77780002e-01,  -1.58969998e-01,   1.20409997e-02,\n",
       "        -5.42230010e-02,  -2.98709989e-01,  -1.57490000e-01,\n",
       "        -3.47579986e-01,  -4.56370004e-02,  -4.42510009e-01,\n",
       "         1.87849998e-01,   2.78489990e-03,  -1.84110001e-01,\n",
       "        -1.15139998e-01,  -7.85809994e-01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks (RNN)\n",
    "\n",
    "![1.png](pictures/1.png)\n",
    "\n",
    "\n",
    "Рекуррентые нейронные сети. Это сети, содержащие обратные связи и позволяющие сохранять информацию.\n",
    "\n",
    "![2.png](pictures/2.png)\n",
    "\n",
    "Иногда для выполнения текущей задачи нам необходима только недавняя информация. Рассмотрим, например, языковую модель, пытающуюся предсказать следующее слово на основании предыдущих. Если мы хотим предсказать последнее слово в предложении “облака плывут по небу”, нам не нужен более широкий контекст; в этом случае довольно очевидно, что последним словом будет “небу”. В этом случае, когда дистанция между актуальной информацией и местом, где она понадобилась, невелика, RNN могут обучиться использованию информации из прошлого. \n",
    "\n",
    "Но бывают случаи, когда нам необходимо больше контекста. Допустим, мы хотим предсказать последнее слово в тексте “Я вырос во Франции… Я бегло говорю по-французски”. Ближайший контекст предполагает, что последним словом будет называние языка, но чтобы установить, какого именно языка, нам нужен контекст Франции из более отдаленного прошлого. Таким образом, разрыв между актуальной информацией и точкой ее применения может стать очень большим.\n",
    "\n",
    "К сожалению, по мере роста этого расстояния, RNN теряют способность связывать информацию.\n",
    "Так называемая __vanishing gradients problem__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 515,201\n",
      "Trainable params: 15,201\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "NAME = \"simple_rnn\"\n",
    "\n",
    "# embedding layer initialization\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# callbacks initialization\n",
    "# stop training model if accuracy does not increase more than five epochs\n",
    "callback_1 = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "# best model saving\n",
    "callback_2 = ModelCheckpoint(\"../models/model_{}.hdf5\".format(NAME), monitor='val_acc',\n",
    "                                 save_best_only=True, verbose=1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_val, labels_val], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory (LSTM)\n",
    "\n",
    "LSTM – необычная модификация рекуррентной нейронной сети, которая на многих задачах значительно превосходит стандартную версию. Почти все впечатляющие результаты RNN достигнуты именно с помощью LSTM. \n",
    "\n",
    "![3.png](pictures/3.png)\n",
    "\n",
    "Проилюстрируем разницу между RNN и  LSTM на следующем примере.\n",
    "\n",
    "<img src=\"http://i.imgur.com/PnWiSCf.png\" alt=\"pokemon_rnn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "As can be seen from the picture, the recurrent network remembers what happened a couple of seconds ago and can roughly understand what caused the appearance of water in the next episode.\n",
    "\n",
    "What \"thinks\" LSTM:\n",
    "<img src=\"http://i.imgur.com/EGZIUuc.png\" alt=\"pokemon_lstm\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "LSTM recalls what happened in the previous episode, also recall long-term memory and focuses only on the right information for a specific episode.\n",
    "\n",
    "Sources:\n",
    "1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "2. [Русская версия на github](https://habrahabr.ru/company/wunderfund/blog/331310/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,501\n",
      "Trainable params: 60,501\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# инициализируем слой эмбеддингов\n",
    "NAME = \"simple_lstm\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_val, labels_val], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked LSTM performes a little bit worse than bidirectional LSTM: train – **79.09%**, validation – **79.80%**.\n",
    "\n",
    "As you can see, models performance is very close, so it worth to try different architectures and regularization techniques to get the best accuracy on a specific task.\n",
    "\n",
    "These approaches are basic in applying recurrent neural networks for the sentiment analysis problem. Improve accuracy can be the following modifications:\n",
    "\n",
    "- size of embeddings;\n",
    "- the number of words in the dictionary;\n",
    "- the maximum long sentence;\n",
    "- modification of the network architecture;\n",
    "- ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использованные материалы: \n",
    "- http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/  \n",
    "- Лекции с  UCU summer school  \n",
    "- Статьи Open Data Science Community  \n",
    "- https://habrahabr.ru/company/wunderfund/blog/331310/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полезные ссылки\n",
    "- [туториал Bag of Words Meets Bags of Popcorn на kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "- [библиотека gensim](https://radimrehurek.com/gensim/index.html)\n",
    "- [Word2Vec Parameter Learning Explained, Xin Rong](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
